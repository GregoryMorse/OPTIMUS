\hypertarget{classAdam}{}\doxysection{Adam Class Reference}
\label{classAdam}\index{Adam@{Adam}}


The \mbox{\hyperlink{classAdam}{Adam}} local optimization method described in \href{https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/}{\texttt{ https\+://machinelearningmastery.\+com/adam-\/optimization-\/algorithm-\/for-\/deep-\/learning/}}.  




{\ttfamily \#include $<$adam.\+h$>$}

\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classAdam_a7523581b9cba940c58adab218a32b50e}{Adam}} (\mbox{\hyperlink{classProblem}{Problem}} $\ast$p)
\item 
double \mbox{\hyperlink{classAdam_a4ab67759e00f08e26665098693acd645}{Solve}} (Data \&x)
\item 
void \mbox{\hyperlink{classAdam_a34a3f8e6e881d51df162bd31923890d6}{set\+B1}} (double b)
\item 
void \mbox{\hyperlink{classAdam_ac3087d0b7f5fbd4228508cd91f3c3284}{set\+B2}} (double b)
\item 
double \mbox{\hyperlink{classAdam_af3f3bfcf691979276b18ff7f082f5d3f}{get\+B1}} () const
\item 
double \mbox{\hyperlink{classAdam_a345f24a12398ec50386dc55b0a0791f7}{get\+B2}} () const
\item 
void \mbox{\hyperlink{classAdam_aaeb2dc13e903afc5756c8a4b3a8afbc1}{set\+Learning\+Rate}} (double r)
\item 
double \mbox{\hyperlink{classAdam_afcab2aa7b88b5f55f7d6f713138385b2}{get\+Learning\+Rate}} () const
\item 
void \mbox{\hyperlink{classAdam_a4f51fa1003eb19dccc69532ab823a02c}{set\+Iterations}} (int it)
\item 
int \mbox{\hyperlink{classAdam_a047c20e4d39d3cc03fcd79b9e83b06fc}{get\+Iterations}} () const
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
The \mbox{\hyperlink{classAdam}{Adam}} local optimization method described in \href{https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/}{\texttt{ https\+://machinelearningmastery.\+com/adam-\/optimization-\/algorithm-\/for-\/deep-\/learning/}}. 

\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classAdam_a7523581b9cba940c58adab218a32b50e}\label{classAdam_a7523581b9cba940c58adab218a32b50e}} 
\index{Adam@{Adam}!Adam@{Adam}}
\index{Adam@{Adam}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{Adam()}{Adam()}}
{\footnotesize\ttfamily Adam\+::\+Adam (\begin{DoxyParamCaption}\item[{\mbox{\hyperlink{classProblem}{Problem}} $\ast$}]{p }\end{DoxyParamCaption})}

Constructor of the Method 
\begin{DoxyParams}{Parameters}
{\em p} & is the problem to be optimized \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{classAdam_af3f3bfcf691979276b18ff7f082f5d3f}\label{classAdam_af3f3bfcf691979276b18ff7f082f5d3f}} 
\index{Adam@{Adam}!getB1@{getB1}}
\index{getB1@{getB1}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{getB1()}{getB1()}}
{\footnotesize\ttfamily double Adam\+::get\+B1 (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the value of the b1 parameter of the \mbox{\hyperlink{classAdam}{Adam}} optimizer 
\end{DoxyReturn}
\mbox{\Hypertarget{classAdam_a345f24a12398ec50386dc55b0a0791f7}\label{classAdam_a345f24a12398ec50386dc55b0a0791f7}} 
\index{Adam@{Adam}!getB2@{getB2}}
\index{getB2@{getB2}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{getB2()}{getB2()}}
{\footnotesize\ttfamily double Adam\+::get\+B2 (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the value of the b2 parameter of the \mbox{\hyperlink{classAdam}{Adam}} optimizer 
\end{DoxyReturn}
\mbox{\Hypertarget{classAdam_a047c20e4d39d3cc03fcd79b9e83b06fc}\label{classAdam_a047c20e4d39d3cc03fcd79b9e83b06fc}} 
\index{Adam@{Adam}!getIterations@{getIterations}}
\index{getIterations@{getIterations}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{getIterations()}{getIterations()}}
{\footnotesize\ttfamily int Adam\+::get\+Iterations (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the value of maximum number of iterations allowed for adam 
\end{DoxyReturn}
\mbox{\Hypertarget{classAdam_afcab2aa7b88b5f55f7d6f713138385b2}\label{classAdam_afcab2aa7b88b5f55f7d6f713138385b2}} 
\index{Adam@{Adam}!getLearningRate@{getLearningRate}}
\index{getLearningRate@{getLearningRate}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{getLearningRate()}{getLearningRate()}}
{\footnotesize\ttfamily double Adam\+::get\+Learning\+Rate (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the learning rate of the \mbox{\hyperlink{classAdam}{Adam}} 
\end{DoxyReturn}
\mbox{\Hypertarget{classAdam_a34a3f8e6e881d51df162bd31923890d6}\label{classAdam_a34a3f8e6e881d51df162bd31923890d6}} 
\index{Adam@{Adam}!setB1@{setB1}}
\index{setB1@{setB1}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{setB1()}{setB1()}}
{\footnotesize\ttfamily void Adam\+::set\+B1 (\begin{DoxyParamCaption}\item[{double}]{b }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em b,the} & value for the b1 parameter of the \mbox{\hyperlink{classAdam}{Adam}} algorithm. b should be between 0 and 1. \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classAdam_ac3087d0b7f5fbd4228508cd91f3c3284}\label{classAdam_ac3087d0b7f5fbd4228508cd91f3c3284}} 
\index{Adam@{Adam}!setB2@{setB2}}
\index{setB2@{setB2}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{setB2()}{setB2()}}
{\footnotesize\ttfamily void Adam\+::set\+B2 (\begin{DoxyParamCaption}\item[{double}]{b }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em b,the} & value for the b2 parameter of the \mbox{\hyperlink{classAdam}{Adam}} algorithm. b should be between 0 and 1. \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classAdam_a4f51fa1003eb19dccc69532ab823a02c}\label{classAdam_a4f51fa1003eb19dccc69532ab823a02c}} 
\index{Adam@{Adam}!setIterations@{setIterations}}
\index{setIterations@{setIterations}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{setIterations()}{setIterations()}}
{\footnotesize\ttfamily void Adam\+::set\+Iterations (\begin{DoxyParamCaption}\item[{int}]{it }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em it} & is the maximum number of iterations for the \mbox{\hyperlink{classAdam}{Adam}} optimizer. it should be \texorpdfstring{$>$}{>}0 \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classAdam_aaeb2dc13e903afc5756c8a4b3a8afbc1}\label{classAdam_aaeb2dc13e903afc5756c8a4b3a8afbc1}} 
\index{Adam@{Adam}!setLearningRate@{setLearningRate}}
\index{setLearningRate@{setLearningRate}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{setLearningRate()}{setLearningRate()}}
{\footnotesize\ttfamily void Adam\+::set\+Learning\+Rate (\begin{DoxyParamCaption}\item[{double}]{r }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em r} & is the learning rate for the \mbox{\hyperlink{classAdam}{Adam}} method. r should be between 0 and 1 \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{classAdam_a4ab67759e00f08e26665098693acd645}\label{classAdam_a4ab67759e00f08e26665098693acd645}} 
\index{Adam@{Adam}!Solve@{Solve}}
\index{Solve@{Solve}!Adam@{Adam}}
\doxysubsubsection{\texorpdfstring{Solve()}{Solve()}}
{\footnotesize\ttfamily double Adam\+::\+Solve (\begin{DoxyParamCaption}\item[{Data \&}]{x }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em x} & is the starting point for the optimizer. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
the estimated minimum 
\end{DoxyReturn}
