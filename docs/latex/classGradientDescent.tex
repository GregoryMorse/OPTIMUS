\hypertarget{classGradientDescent}{}\doxysection{Gradient\+Descent Class Reference}
\label{classGradientDescent}\index{GradientDescent@{GradientDescent}}


{\ttfamily \#include $<$gradientdescent.\+h$>$}

\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classGradientDescent_a6a5c849c256c080dfda7ce05632f9d4d}{Gradient\+Descent}} (\mbox{\hyperlink{classProblem}{Problem}} $\ast$p)
\item 
double \mbox{\hyperlink{classGradientDescent_a1b82c35c7dc3406efedfab1d9a9ed432}{Solve}} (Data \&x)
\item 
void \mbox{\hyperlink{classGradientDescent_a3251e361b931997c772b9d0e1af3e4df}{set\+Rate}} (double r)
\item 
double \mbox{\hyperlink{classGradientDescent_a77ef0f786a7ed57407006e9e47001ce3}{get\+Rate}} () const
\item 
void \mbox{\hyperlink{classGradientDescent_adc4d15f7772c2ea10400f0cbf69840ee}{set\+Iters}} (int i)
\item 
int \mbox{\hyperlink{classGradientDescent_aa97283e68c0c38e5e1678ffd801b9e8a}{get\+Iters}} () const
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Implement the steepest descent optimization method 

Definition at line 5 of file gradientdescent.\+h.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classGradientDescent_a6a5c849c256c080dfda7ce05632f9d4d}\label{classGradientDescent_a6a5c849c256c080dfda7ce05632f9d4d}} 
\index{GradientDescent@{GradientDescent}!GradientDescent@{GradientDescent}}
\index{GradientDescent@{GradientDescent}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{GradientDescent()}{GradientDescent()}}
{\footnotesize\ttfamily Gradient\+Descent\+::\+Gradient\+Descent (\begin{DoxyParamCaption}\item[{\mbox{\hyperlink{classProblem}{Problem}} $\ast$}]{p }\end{DoxyParamCaption})}

The constructor of the class. The
\begin{DoxyParams}{Parameters}
{\em p} & is the objective problem to be minimized \\
\hline
\end{DoxyParams}


Definition at line 3 of file gradientdescent.\+cpp.



\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{classGradientDescent_aa97283e68c0c38e5e1678ffd801b9e8a}\label{classGradientDescent_aa97283e68c0c38e5e1678ffd801b9e8a}} 
\index{GradientDescent@{GradientDescent}!getIters@{getIters}}
\index{getIters@{getIters}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{getIters()}{getIters()}}
{\footnotesize\ttfamily int Gradient\+Descent\+::get\+Iters (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the number of maximum iteratiosn allowed 
\end{DoxyReturn}


Definition at line 48 of file gradientdescent.\+cpp.

\mbox{\Hypertarget{classGradientDescent_a77ef0f786a7ed57407006e9e47001ce3}\label{classGradientDescent_a77ef0f786a7ed57407006e9e47001ce3}} 
\index{GradientDescent@{GradientDescent}!getRate@{getRate}}
\index{getRate@{getRate}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{getRate()}{getRate()}}
{\footnotesize\ttfamily double Gradient\+Descent\+::get\+Rate (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}

\begin{DoxyReturn}{Returns}
the learning rate of the steepest descent 
\end{DoxyReturn}


Definition at line 37 of file gradientdescent.\+cpp.

\mbox{\Hypertarget{classGradientDescent_adc4d15f7772c2ea10400f0cbf69840ee}\label{classGradientDescent_adc4d15f7772c2ea10400f0cbf69840ee}} 
\index{GradientDescent@{GradientDescent}!setIters@{setIters}}
\index{setIters@{setIters}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{setIters()}{setIters()}}
{\footnotesize\ttfamily void Gradient\+Descent\+::set\+Iters (\begin{DoxyParamCaption}\item[{int}]{i }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em i} & is the required maximum number of iterations allowed for the steepest descent method \\
\hline
\end{DoxyParams}


Definition at line 42 of file gradientdescent.\+cpp.

\mbox{\Hypertarget{classGradientDescent_a3251e361b931997c772b9d0e1af3e4df}\label{classGradientDescent_a3251e361b931997c772b9d0e1af3e4df}} 
\index{GradientDescent@{GradientDescent}!setRate@{setRate}}
\index{setRate@{setRate}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{setRate()}{setRate()}}
{\footnotesize\ttfamily void Gradient\+Descent\+::set\+Rate (\begin{DoxyParamCaption}\item[{double}]{r }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em r} & is the required learning rate for the steepest descent method. r should be between 0 and 1. \\
\hline
\end{DoxyParams}


Definition at line 32 of file gradientdescent.\+cpp.

\mbox{\Hypertarget{classGradientDescent_a1b82c35c7dc3406efedfab1d9a9ed432}\label{classGradientDescent_a1b82c35c7dc3406efedfab1d9a9ed432}} 
\index{GradientDescent@{GradientDescent}!Solve@{Solve}}
\index{Solve@{Solve}!GradientDescent@{GradientDescent}}
\doxysubsubsection{\texorpdfstring{Solve()}{Solve()}}
{\footnotesize\ttfamily double Gradient\+Descent\+::\+Solve (\begin{DoxyParamCaption}\item[{Data \&}]{x }\end{DoxyParamCaption})}


\begin{DoxyParams}{Parameters}
{\em x} & is the starting point of the optimization procedure \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
the estimated minimum 
\end{DoxyReturn}


Definition at line 10 of file gradientdescent.\+cpp.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
Optimus/gradientdescent.\+h\item 
Optimus/gradientdescent.\+cpp\end{DoxyCompactItemize}
