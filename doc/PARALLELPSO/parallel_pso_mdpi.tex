%% LyX 2.3.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{mdpi}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amstext}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{New ideas in parallel Particle Swarm Optimization}

\TitleCitation{New ideas in parallel Particle Swarm Optimization}

\Author{Vasileios Charilogis$^{2}$, Ioannis G. Tsoulos$^{1,*}$ }

\AuthorNames{Ioannis G. Tsoulos, Vasileios Charilogis }

\AuthorCitation{Tsoulos, I.G.; Charilogis, V.}


\address{$^{1}$\quad{}Department of Informatics and Telecommunications,
University of Ioannina, Greece;itsoulos@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; v.charilogis@uoi.gr}


\corres{Correspondence: itsoulos@uoi.gr; }


\abstract{In global optimization there are techniques where they find the optimal
solutions of the objective problems, but they waste a lot of computational
time. The PSO parallelization technique proposed in this paper significantly
reduces the computation time and at the same time participates in
the solution finding algorithm by iterative communication between
the parallel computing units. Apart from the sequential algorithm,
the communication strategies 1to1, 1toN, Nto1 and NtoN are compared
where each computing unit sends its knowledge to the other clusters.
In addition, a new and more appropriate termination rule is proposed
here. From the results of the experiments, it appears that the overall
parallelization technique is more than an accelerator of the classical
algorithm.}


\keyword{Optimization, Parallel methods, Evolutionary techniques, Stochastic
methods, Termination rules.}

\DeclareTextSymbolDefault{\textquotedbl}{T1}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================


% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, alloys, analytica, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, currophthalmol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, kidneydial, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands
\firstpage{1} 
 
\setcounter{page}{\@firstpage} 

\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2022}
\copyrightyear{2022}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{} 
\dateaccepted{} 
\datepublished{} 
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\maketitle

\section{Introduction}

The global optimization problem is usually defined as: 
\begin{equation}
x^{*}=\mbox{arg}\min_{x\in S}f(x)\label{eq:eq1}
\end{equation}
with $S$: 
\[
S=\left[a_{1},b_{1}\right]\otimes\left[a_{2},b_{2}\right]\otimes\ldots\left[a_{n},b_{n}\right]
\]
where the function f is assumed to be continuous and differentiable.
Many problems faced by researchers can be formulated as global minimization
problems such as problems in physical science \citep{go_physics1,go_physics2,go_physics3},
chemistry \citep{go_chem1,go_chem2,go_chem3}, economics \citep{go_econ1,go_econ2}
and medicine \citep{go_med1,go_med2}. The global optimization methods
are usually divided into two major categories: deterministic and stochastic
methods. In the deterministic category, the most common method is
the so - called interval method \citep{interval1,interval2}, where
the set $S$ is iteratively divided into subregions and those that
do not contain a global solution are discarded using certain criteria.
In the case of stochastic methods, the finding of the global minimum
is based on randomness operations, although there is no guarantee
of locating the global minimum. Nevertheless, it is the category of
methods that is often used due to the simplicity and the effectiveness
provided. Several researchers have proposed stochastic methods such
as: controlled random search methods\citep{crs1,crs2,crs3}, simulated
annealing methods \citep{simann_major,simann1,simann2}, differential
evolution methods \citep{diffe1,diffe2}, particle swarm optimization
methods \citep{pso_major,pso1,pso2}, Ant Colony Optimization \citep{aco1,aco2},
Genetic Algorithms \citep{ga1,ga2,ga3}, etc. Also, recently, many
studies have appeared that utilize the modern parallel processing
units \citep{gpu1,gpu2,gpu3} to tackle the global optimization problem.
Some research that one can study regarding metaheuristic algorithms
is presented in some recent papers \citep{meta1,meta2,meta3}. This
paper suggests a number of directions for efficient parallelization
of particle swarm optimization (PSO) techniques.

The PSO method is inspired by the observations of Eberhart and Kennedy
in the 1990s. The electrical engineer Russell C. Eberhart and social
psychologist James Kennedy, observed the behavior of birds looking
for food, presented a technique where the atoms or otherwise \textquotedbl particles\textquotedbl{}
fly through the search space seeking for the best position that minimizes
or maximizes a problem. These particles have two basic characteristics:
their position at any instant of time, which is referred to as $\overrightarrow{x}$
and the speed at which they are moving, which is referred to as $\overrightarrow{u}$.
The purpose of this method is to move the particles iteratively and
calculate their next position based on three elements: the current
position, the best position they had in the past and the best position
of the population. The PSO method was successfully used in a variety
of scientific and practical problems in physics\textbf{ }\citep{psophysics1,psophysics2},
chemistry \citep{psochem1,psochem2}, medicine \citep{psomed1,psomed2},
economics \citep{psoecon} etc. Due to its high popularity, the method
has received a number of interventions in recent years, such as combination
with the mutation mechanism \citep{pso_mutation1,pso_mutation2,pso_mutation3},
improved initialization of the velocity vector \citep{pso_initvelocity},\textbf{
}hybrid techniques \citep{psohybrid1,psohybrid2,psohybrid3}, parallel
techniques \citep{pso_parallel1,pso_parallel2,pso_parallel3}, methods
aim to improve the velocity calculation \citep{pso_velocity1,pso_velocity2,pso_velocity3}
etc. The method of PSO has been integrated into other optimization
techniques like the work of Bogdanova et al \citep{ge_pso1} who combined
Grammatical Evolution with swarm techniques like PSO \citep{ge_mainpaper},
the work of Pan et al \citep{siman_pso} to create a hybrid PSO method
with simulated annealing. Also, Mughal et al \citep{simman_pso_energy}
used a hybrid technique of PSO and Simulated Annealing for photovoltaic
cell parameter estimation. Similarly, the work of Lin et al \citep{pso_de}
utilized a hybrid method of PSO and Differential Evolution for numerical
optimization problems. Variations of PSO that aim at the global minimum
in a shorter time may include the use of a local optimization method
in each iteration of the algorithm \citep{pso_local1,pso_local2}.
Of course, the above process can be extremely time consuming and,
depending on the termination method used and the number of local searches
performed, may require a long execution time. 

The proposed method creates a number of sub-populations of particles
that run independently on parallel computing units that will also
be called islands. Also, a series of modifications to the original
particle swarm optimization method are proposed in order to make it
more efficient in parallel computing environments. These modifications
include a new method of calculating particle velocity, a new termination
rule specifically modified for parallel techniques, and a new way
of propagating the best particles among the parallel computing units
involved in the overall method.

The rest of this article is organized as follows: in section \ref{sec:The-proposed-method}
the proposed method and the new approaches in Particle Swarm Optimization
are discussed in detail, in section \ref{sec:Experiments} the used
test functions as well the experimental results are fully outlined
and finally in section \ref{sec:Conclusions} some conclusions and
future guidelines are listed.

\section{The proposed method\label{sec:The-proposed-method}}

In this section the discussion will begin with the steps of the serial
method as well as a general outline of the parallel technique followed.
Then the basic components of the proposed process, such as the calculation
of the speed, the proposed termination rule and the method of propagating
points between the parallel computing units will be thoroughly analyzed.

\subsection{The base algorithm\label{subsec:The-base-algorithm}}

The base PSO algorithm executed in every parallel processing unit
is listed in Algorithm \ref{alg:psoSerial}.

\begin{algorithm}[H]
\caption{The base PSO algorithm executed in one processing unit.\label{alg:psoSerial}}

\begin{enumerate}
\item \textbf{Initialization Step} . 

\begin{enumerate}
\item \textbf{Set} $\text{\mbox{iter}}=0$.
\item \textbf{Set} $m$ as the total number of particles.
\item \textbf{Set }$\mbox{iter}_{\mbox{max}}$ as the maximum number of
allowed generations.
\item \textbf{Set} randomly, the initial positions of the $m$ particles
$x_{1},x_{2},...,x_{m}$.
\item \textbf{Initialize} randomly the velocities $u_{1},u_{2},...,u_{m}$.
\item \textbf{For} $i=1..m$ do $p_{i}=x_{i}$. The vector $p_{i}$ stands
for the best located position of particle $i$.
\item \textbf{Set} $p_{\mbox{best}}=\arg\min_{i\in1..m}f\left(x_{i}\right)$
\end{enumerate}
\item \textbf{Termination Check Step} .\label{enu:Check-Termination.} If
the termination criteria are hold, then terminate.
\item \textbf{For} $i=1..m$ \textbf{Do\label{enu:For}}

\begin{enumerate}
\item \textbf{Compute }the velocity $u_{i}$ using $u_{i},\ p_{i}$ and
$p_{\mbox{best}}$
\item \textbf{Set} the new position $x_{i}=x_{i}+u_{i}$\label{enu:Update-the-position}
\item \textbf{Calculate} the $f\left(x_{i}\right)$ for particle $x_{i}$
\item \textbf{If} $f\left(x_{i}\right)\le f\left(p_{i}\right)$ then $p_{i}=x_{i}$
\end{enumerate}
\item \textbf{End} \textbf{For}
\item \textbf{Set} $p_{\mbox{best}}=\arg\min_{i\in1..m}f\left(x_{i}\right)$
\item \textbf{Set} $\mbox{iter}=\mbox{iter}+1$. \label{enu:update_k}
\item \textbf{Goto} Step \ref{enu:Check-Termination.}
\end{enumerate}
\end{algorithm}
The base PSO algorithm described in Algorithm \ref{alg:psoSerial}
calculates at every iteration the new position $x_{i}$ with the following
operation:
\begin{equation}
x_{i}=x_{i}+u_{i}\label{eq:eq3}
\end{equation}
In most cases the new speed is a linear combination of the old speed
and the best values $p_{i}$ and $p_{\mbox{best}}$ and it is defined
as:
\begin{equation}
u_{i}=\omega u_{i}+r_{1}c_{1}\left(p_{i}-x_{i}\right)+r_{2}c_{2}\left(p_{\mbox{best}}-x_{i}\right)\label{eq:eq4-1}
\end{equation}
where 
\begin{enumerate}
\item The variables $r_{1},\ r_{2}$ are random numbers defined in $[0,1].$
\item The constant number $c_{1},\ c_{2}$ are in the range $[1,2]$. 
\item The variable $\omega$ is commonly called inertia and typically $\omega\in[0,1]$.
The inertia was proposed by Shi and Eberhart \citep{pso_major}. In
the current article the same inertia calculation as proposed in \citep{ipso}
is used. The inertia is calculated through the following equation:
\end{enumerate}
\begin{equation}
\omega_{\mbox{iter}}=0.5+\frac{r}{2}
\end{equation}
with $r$ being a random number and $r\in[0,1]$. 

\subsection{The parallel algorithm}

The overall parallel algorithm, which runs on $N_{I}$ independent
computing units, is shown in algorithm \ref{alg:parallelPso}.

\begin{algorithm}[H]
\caption{The implemented parallel algorithm.\label{alg:parallelPso}}

\begin{enumerate}
\item \textbf{Set} as $N_{I}$ the total number of parallel processing units.
\item \textbf{Set} as $N_{R}$ as the number of iterations, after which
each processing unit will send its best particles to the remaining
processing units.
\item \textbf{Set} $N_{P}$ the number of migrated particles between the
parallel processing units.
\item \textbf{Set} $K=0$ the iteration number.
\item \textbf{For} $j=1,..,N$ do in parallel\label{enu:For--do}
\begin{enumerate}
\item \textbf{Execute} an iteration of the PSO algorithm described in algorithm
\ref{alg:psoSerial} on processing unit $j$.
\item \textbf{If $K\ \mbox{mod}\ N_{R}=0,$then }
\begin{enumerate}
\item \textbf{Get} the best $N_{P}$ particles from algorithm $j$.
\item \textbf{Propagate} these $N_{P}$ particles to the rest of processing
units using some propagation scheme that will be described subsequently.
\end{enumerate}
\item EndIf
\end{enumerate}
\item \textbf{End For}
\item \textbf{Update} $K=K+1$
\item \textbf{Check} the termination rule. If the termination rule holds
then goto step \ref{enu:Terminate-and-report} else goto step \ref{enu:For--do}.
\item \textbf{Terminate} and report the best value from all processing units.\label{enu:Terminate-and-report}.
Apply a local search procedure to this located value to enchance the
located global minimum. . In the proposed algorithm a BFGS variation
of Powell \citep{Powell} was used a local search procedure.
\end{enumerate}
\end{algorithm}
The main aspects of the parallel algorithm are the propagation mechanism
and the proposed termination rule, that is properly adjusted to the
parallel computation environment. These aspects will be discussed
in the following subsections.

\subsection{Propagation mechanism \label{subsec:Propagation-mechanism}}

During the execution of the parallel algorithm and periodically, the
processing units propagate their best particles (those with the lowest
value in the objective function) to the remaining processing units.
This dissemination can be done in the following possible ways:
\begin{enumerate}
\item \textbf{1 to 1}. In this propagation scheme, a randomly selected processing
unit will send to some other randomly selected unit its $N_{P}$ best
particles.
\item \textbf{1 to N}. During this scheme, a randomly selected unit will
send its best $N_{P}$ particles to the remaining units.
\item \textbf{N to 1}. In this scheme, all processing units will send the
corresponding $N_{P}$ best particles of each unit to a randomly selected
unit.
\item \textbf{N to N}. For this scheme, all processing units will send the
corresponding $N_{P}$ best particles to all.
\end{enumerate}
All propagation schemes are demonstrated graphically in Figure \ref{fig:allSchemes}.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.7]{image2}
\par\end{centering}
\caption{A graphic presentation of all propagation schemes.\label{fig:allSchemes}}
\end{figure}


\subsection{Stopping rule}

In the proposed technique, a distinct termination rule is checked
on each parallel processing unit. This rule is formulated as follows:
\begin{equation}
\delta_{i}^{(k)}=\left|f_{i,\mbox{min}}^{(k)}-f_{i,\mbox{min}}^{(k-1)}\right|\label{eq:term}
\end{equation}
This quantity is calculated on every iteration $k$. The value $f_{i,\mbox{min}}^{(k)}$
is the best located function value for unit $i$ at iteration $k$.
If the above quantity is less than a predetermined limit $\epsilon$
for $N_{M}$ continuous repetitions, then the algorithm executed on
this unit is terminated. In present work, if a parallel processing
unit is terminated, then the overall process is also terminated.

\section{Experiments \label{sec:Experiments}}

To measure the reliability and efficiency of the proposed technique,
experiments were performed on a wide range of objective functions
from the relevant literature\citep{Ali1,Floudas1}, which have been
studied by many researchers\citep{testfunctions1,testfunctions2,testfunctions3,testfunctions4}.
In these experiments the ability of the method to find the global
minimum was measured and also a study of the basic parameters of the
proposed technique was made.

\subsection{Test functions}

The definition of the test functions used are given below
\begin{itemize}
\item \textbf{\emph{Bent Cigar function}}\emph{ }The function is 
\[
f(x)=x_{1}^{2}+10^{6}\sum_{i=2}^{n}x_{i}^{2}
\]
The value $n=10$ was used in the conducted experiments.
\item \textbf{Bf1} (Bohachevsky 1) function:
\end{itemize}
\[
f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)-\frac{4}{10}\cos\left(4\pi x_{2}\right)+\frac{7}{10}
\]
where $x\in[-100,100]^{2}$. 
\begin{itemize}
\item \textbf{Bf2} (Bohachevsky 2) function: 
\[
f(x)=x_{1}^{2}+2x_{2}^{2}-\frac{3}{10}\cos\left(3\pi x_{1}\right)\cos\left(4\pi x_{2}\right)+\frac{3}{10}
\]
where $x\in[-50,50]^{2}$. 
\item \textbf{Branin} function: $f(x)=\left(x_{2}-\frac{5.1}{4\pi^{2}}x_{1}^{2}+\frac{5}{\pi}x_{1}-6\right)^{2}+10\left(1-\frac{1}{8\pi}\right)\cos(x_{1})+10$
with $-5\le x_{1}\le10,\ 0\le x_{2}\le15$. 
\item \textbf{CM} function: 
\[
f(x)=\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{10}\sum_{i=1}^{n}\cos\left(5\pi x_{i}\right)
\]
where $x\in[-1,1]^{n}$. The value $n=4$ was used in the conducted
experiments.
\item \textbf{Camel} function:
\[
f(x)=4x_{1}^{2}-2.1x_{1}^{4}+\frac{1}{3}x_{1}^{6}+x_{1}x_{2}-4x_{2}^{2}+4x_{2}^{4},\quad x\in[-5,5]^{2}
\]
\item \textbf{Discus function}\emph{ }The function:
\[
f(x)=10^{6}x_{1}^{2}+\sum_{i=2}^{n}x_{i}^{2}
\]
The value $n=10$ was used in the conducted experiments.
\item \textbf{Easom} function: 
\[
f(x)=-\cos\left(x_{1}\right)\cos\left(x_{2}\right)\exp\left(\left(x_{2}-\pi\right)^{2}-\left(x_{1}-\pi\right)^{2}\right)
\]
with $x\in[-100,100]^{2}.$ 
\item \textbf{Exponential} function, defined as: 
\[
f(x)=-\exp\left(-0.5\sum_{i=1}^{n}x_{i}^{2}\right),\quad-1\le x_{i}\le1
\]
 The values $n=4,16,64$ were used in the executed experiments.
\item \textbf{Griewank2} function:
\[
f(x)=1+\frac{1}{200}\sum_{i=1}^{2}x_{i}^{2}-\prod_{i=1}^{2}\frac{\cos(x_{i})}{\sqrt{(i)}},\quad x\in[-100,100]^{2}
\]
\item \textbf{Gkls} function. $f(x)=\mbox{Gkls}(x,n,w)$, is a function
with $w$ local minima, described in \citep{gkls} with $x\in[-1,1]^{n}$
and $n$ a positive integer between 2 and 100. The value of the global
minimum is -1 and in our experiments we have used $n=2,3$ and $w=50,\ 100$. 
\item \textbf{Hansen} function: $f(x)=\sum_{i=1}^{5}i\cos\left[(i-1)x_{1}+i\right]\sum_{j=1}^{5}j\cos\left[(j+1)x_{2}+j\right]$,
$x\in[-10,10]^{2}$ .
\item \textbf{Hartman 3} function:
\[
f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{3}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)
\]
with $x\in[0,1]^{3}$ and $a=\left(\begin{array}{ccc}
3 & 10 & 30\\
0.1 & 10 & 35\\
3 & 10 & 30\\
0.1 & 10 & 35
\end{array}\right),\ c=\left(\begin{array}{c}
1\\
1.2\\
3\\
3.2
\end{array}\right)$ and
\[
p=\left(\begin{array}{ccc}
0.3689 & 0.117 & 0.2673\\
0.4699 & 0.4387 & 0.747\\
0.1091 & 0.8732 & 0.5547\\
0.03815 & 0.5743 & 0.8828
\end{array}\right)
\]
\item \textbf{Hartman 6} function:
\[
f(x)=-\sum_{i=1}^{4}c_{i}\exp\left(-\sum_{j=1}^{6}a_{ij}\left(x_{j}-p_{ij}\right)^{2}\right)
\]
with $x\in[0,1]^{6}$ and $a=\left(\begin{array}{cccccc}
10 & 3 & 17 & 3.5 & 1.7 & 8\\
0.05 & 10 & 17 & 0.1 & 8 & 14\\
3 & 3.5 & 1.7 & 10 & 17 & 8\\
17 & 8 & 0.05 & 10 & 0.1 & 14
\end{array}\right),\ c=\left(\begin{array}{c}
1\\
1.2\\
3\\
3.2
\end{array}\right)$ and
\[
p=\left(\begin{array}{cccccc}
0.1312 & 0.1696 & 0.5569 & 0.0124 & 0.8283 & 0.5886\\
0.2329 & 0.4135 & 0.8307 & 0.3736 & 0.1004 & 0.9991\\
0.2348 & 0.1451 & 0.3522 & 0.2883 & 0.3047 & 0.6650\\
0.4047 & 0.8828 & 0.8732 & 0.5743 & 0.1091 & 0.0381
\end{array}\right)
\]
\item \textbf{Potential} function. The molecular conformation corresponding
to the global minimum of the energy of N atoms interacting via the
Lennard-Jones potential\citep{Jones} is used a test function here
and it is defined by:
\begin{equation}
V_{LJ}(r)=4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^{6}\right]\label{eq:potential}
\end{equation}
For our experiments we used: $N=3,\ 5$
\item \textbf{Rastrigin} function. 
\[
f(x)=x_{1}^{2}+x_{2}^{2}-\cos(18x_{1})-\cos(18x_{2}),\quad x\in[-1,1]^{2}
\]
\item \textbf{\emph{Rosenbrock}}\emph{ function}.\\
\[
f(x)=\sum_{i=1}^{n-1}\left(100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(x_{i}-1\right)^{2}\right),\quad-30\le x_{i}\le30.
\]
In our experiments we used the values $n=4,\ 8$.
\item \textbf{Shekel 7} function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{7}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7\\
2 & 9 & 2 & 9\\
5 & 3 & 5 & 3
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4\\
0.6\\
0.3
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Shekel 5 }function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{5}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]
 

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Shekel 10} function.
\end{itemize}
\[
f(x)=-\sum_{i=1}^{10}\frac{1}{(x-a_{i})(x-a_{i})^{T}+c_{i}}
\]
 

with $x\in[0,10]^{4}$ and $a=\left(\begin{array}{cccc}
4 & 4 & 4 & 4\\
1 & 1 & 1 & 1\\
8 & 8 & 8 & 8\\
6 & 6 & 6 & 6\\
3 & 7 & 3 & 7\\
2 & 9 & 2 & 9\\
5 & 5 & 3 & 3\\
8 & 1 & 8 & 1\\
6 & 2 & 6 & 2\\
7 & 3.6 & 7 & 3.6
\end{array}\right),\ c=\left(\begin{array}{c}
0.1\\
0.2\\
0.2\\
0.4\\
0.4\\
0.6\\
0.3\\
0.7\\
0.5\\
0.6
\end{array}\right)$. 
\begin{itemize}
\item \textbf{Sinusoidal} function: 
\[
f(x)=-\left(2.5\prod_{i=1}^{n}\sin\left(x_{i}-z\right)+\prod_{i=1}^{n}\sin\left(5\left(x_{i}-z\right)\right)\right),\quad0\le x_{i}\le\pi.
\]
The values of $n=4,8$ and $z=\frac{\pi}{6}$ was used in the experimental
results.
\item \textbf{Test2N} function:
\[
f(x)=\frac{1}{2}\sum_{i=1}^{n}x_{i}^{4}-16x_{i}^{2}+5x_{i},\quad x_{i}\in[-5,5].
\]
The function has $2^{n}$ in the specified range and in our experiments
we used $n=4,5,6,7$.
\item \textbf{Test30N} function:
\[
f(x)=\frac{1}{10}\sin^{2}\left(3\pi x_{1}\right)\sum_{i=2}^{n-1}\left(\left(x_{i}-1\right)^{2}\left(1+\sin^{2}\left(3\pi x_{i+1}\right)\right)\right)+\left(x_{n}-1\right)^{2}\left(1+\sin^{2}\left(2\pi x_{n}\right)\right)
\]
with $x\in[-10,10]$, with $30^{n}$ local minima in the search space.
For our experiments we used $n=3,4$.
\end{itemize}

\subsection{Experimental results}

The proposed method was tested on the previously mentioned test functions.
Every experiment mentioned was executed 30 times and the average number
of function calls was reported. The most critical parameters of the
proposed method are listed in table \ref{tab:values}.

\begin{table}[H]
\caption{The values for most critical parameters of the algorithm.\label{tab:values}}

\centering{}%
\begin{tabular}{|c|c|}
\hline 
PARAMETER & VALUE\tabularnewline
\hline 
\hline 
$m$ & 200\tabularnewline
\hline 
$\mbox{iter}_{\mbox{max}}$ & 200\tabularnewline
\hline 
$c_{1}$ & 1.0\tabularnewline
\hline 
$c_{2}$ & 1.0\tabularnewline
\hline 
$N_{R}$ & 15\tabularnewline
\hline 
$\epsilon$ & $10^{-6}$\tabularnewline
\hline 
$N_{M}$ & 15\tabularnewline
\hline 
\end{tabular}
\end{table}
The method was compared against a genetic algorithm and a simple PSO
method with the same set of parameters (chromosomes and particles)
and the results are reported in Table \ref{tab:Comparison}. 

\begin{table}[H]
\caption{Comparison of the proposed method against two other global optimization
techniques. The number of processing units is set to $N_{I}=1$.\label{tab:Comparison}}

\raggedright{}%
\begin{tabular}{|c|c|c|c|}
\hline 
Function & GENETIC & PSO & PROPOSED with $N_{I}=1$\tabularnewline
\hline 
\hline 
BF1 & 9581 & 19144(0.83) & 12625\tabularnewline
\hline 
BF2 & 10014 & 19121(0.90) & 13108\tabularnewline
\hline 
BRANIN & 9289 & 17760 & 8574\tabularnewline
\hline 
CIGAR10 & 40226 & 39553 & 40274\tabularnewline
\hline 
CM4 & 15360 & 22829 & 11512\tabularnewline
\hline 
DISCUS10 & 40216 & 22359 & 37848\tabularnewline
\hline 
EASOM & 9994 & 2897 & 4608\tabularnewline
\hline 
ELP10 & 40273 & 31192 & 23436\tabularnewline
\hline 
EXP4 & 14084 & 21375 & 9062\tabularnewline
\hline 
EXP16 & 40215 & 27755 & 22408\tabularnewline
\hline 
EXP64 & 40237 & 26155 & 40238\tabularnewline
\hline 
GKLS250 & 8361 & 16217 & 8070\tabularnewline
\hline 
GKLS350 & 12697(0.97) & 20393 & 10696(0.97)\tabularnewline
\hline 
GRIEWANK2 & 9298(0.97) & 20004(0.87) & 11064\tabularnewline
\hline 
POTENTIAL3 & 27799 & 20876 & 12876\tabularnewline
\hline 
POTENTIAL5 & 40240 & 25809 & 38377\tabularnewline
\hline 
HANSEN & 14951(0.93) & 16945 & 12467\tabularnewline
\hline 
HARTMAN3 & 11268 & 22259 & 10018\tabularnewline
\hline 
HARTMAN6 & 21396(0.63) & 33679(0.33) & 15082(0.53)\tabularnewline
\hline 
RASTRIGIN & 8967 & 16044 & 9286(0.93)\tabularnewline
\hline 
ROSENBROCK4 & 40233 & 26367 & 25120\tabularnewline
\hline 
ROSENBROCK8 & 40271 & 32750 & 38577\tabularnewline
\hline 
SHEKEL5 & 19403(0.70) & 29079(0.33) & 15409(0.43)\tabularnewline
\hline 
SHEKEL7 & 19376(0.80) & 27817(0.47) & 14989(0.63)\tabularnewline
\hline 
SHEKEL10 & 19829(0.77) & 26479(0.83) & 15087(0.67)\tabularnewline
\hline 
SINU4 & 15788 & 23915 & 12298\tabularnewline
\hline 
SINU8 & 30928 & 27834(0.97) & 15500\tabularnewline
\hline 
TEST2N4 & 17109 & 23983(0.97) & 14520(0.70)\tabularnewline
\hline 
TEST2N5 & 19464 & 30817 & 14801(0.47)\tabularnewline
\hline 
TEST2N6 & 24217 & 29067(0.90) & 17444(0.23)\tabularnewline
\hline 
TEST2N7 & 26824 & 32337(0.60) & 22780(0.23)\tabularnewline
\hline 
TEST30N3 & 17575 & 15660 & 7814\tabularnewline
\hline 
TEST30N4 & 17395 & 23519 & 8014\tabularnewline
\hline 
\textbf{TOTAL} & \textbf{732878(0.96)} & \textbf{791990(0.91)} & \textbf{573980(0.87)}\tabularnewline
\hline 
\end{tabular}
\end{table}
In the table, each number in each cell represents the average of the
function values for 30 independent runs. Also, the numbers in parentheses
represent the percentage of runs in which the global minimum was successfully
found. If this percentage is not present, it implies 100\% success.
In addition, a line has been added at the end of the table showing
the total number of function calls for each method. From the experimental
results, it is evident that the proposed technique significantly reduces
the required number of function calls even if it is executed on a
single processing unit.

In order to evaluate the effect of executing the method on parallel
processing units, another experiment was done in which the number
of parallel processing units was increased from 1 to 10 and the results
are presented in Table \ref{tab:t1results}. Also, a box plot for
this experiment is shown in Figure \ref{fig:boxplot1to1}. In order
to have reliability in the measurements, the total number of particles
remained constant as the number of units increased. So, for example,
in the case of the two computing units, in each unit the particles
were 100 while in 5 units the particles were 40. In this way, the
total number of particles used remains constant at 200.

\begin{table}[H]
\caption{Experimental results using the proposed method, the propagation scheme
was set to 1to1 and the value of $N_{P}$ was set to 5. In the conducted
experiments the number of parallel processing units was varyied from
1 to 10.\label{tab:t1results}}

\raggedright{}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Function & $N_{I}=1$ & $N_{I}=2$ & $N_{I}=4$ & $N_{I}=5$ & $N_{I}=10$\tabularnewline
\hline 
\hline 
BF1 & 12625 & 11660 & 9984 & 10315 & 6667\tabularnewline
\hline 
BF2 & 13108 & 11600 & 10363 & 9403 & 6964\tabularnewline
\hline 
BRANIN & 8574 & 6953 & 5412 & 5170 & 4141\tabularnewline
\hline 
CIGAR10 & 40274 & 40180 & 39763 & 38887 & 21291\tabularnewline
\hline 
CM4 & 11512 & 12019 & 12203 & 12339 & 9910\tabularnewline
\hline 
DISCUS10 & 37848 & 26044 & 13211 & 10989 & 4171\tabularnewline
\hline 
EASOM & 4608 & 3927 & 3660 & 3513 & 3110\tabularnewline
\hline 
ELP10 & 23436 & 26469 & 14268 & 11100 & 7462\tabularnewline
\hline 
EXP4 & 9062 & 9691 & 9678 & 9556 & 9431\tabularnewline
\hline 
EXP16 & 22408 & 15608 & 18025 & 21307 & 21991\tabularnewline
\hline 
EXP64 & 40238 & 40177 & 39856 & 39731 & 24234\tabularnewline
\hline 
GKLS250 & 8070 & 7809 & 7225 & 6853 & 5591\tabularnewline
\hline 
GKLS350 & 10696(0.97) & 11488 & 10578 & 10095 & 7279\tabularnewline
\hline 
GRIEWANK2 & 11064 & 10681 & 9127 & 8926 & 5604\tabularnewline
\hline 
POTENTIAL3 & 12876 & 5568 & 5018 & 4756 & 4333\tabularnewline
\hline 
POTENTIAL5 & 38377 & 4905 & 4455 & 4221 & 4016\tabularnewline
\hline 
HANSEN & 12467 & 5067 & 4340 & 4031 & 3518\tabularnewline
\hline 
HARTMAN3 & 10018 & 10263 & 10162 & 9711 & 8234\tabularnewline
\hline 
HARTMAN6 & 15082(0.53) & 9816(0.73) & 7212(0.97) & 7194 & 5935\tabularnewline
\hline 
RASTRIGIN & 9286(0.93) & 9432 & 6227 & 5974 & 4254\tabularnewline
\hline 
ROSENBROCK4 & 25120 & 20084 & 16454 & 12244 & 7574\tabularnewline
\hline 
ROSENBROCK8 & 38577 & 25195 & 22531 & 18508 & 9587\tabularnewline
\hline 
SHEKEL5 & 15409(0.43) & 14112(0.77) & 8575(0.87) & 7898(0.93) & 4948\tabularnewline
\hline 
SHEKEL7 & 14989(0.63) & 13800(0.90) & 9227(0.93) & 8717(0.97) & 5050\tabularnewline
\hline 
SHEKEL10 & 15087(0.67) & 14662(0.87) & 10268(0.93) & 8229(0.93) & 4871(0.97)\tabularnewline
\hline 
SINU4 & 12298 & 12997 & 13172 & 12842 & 10316\tabularnewline
\hline 
SINU8 & 15500 & 15475 & 16442 & 16375 & 12732\tabularnewline
\hline 
TEST2N4 & 14520(0.70) & 15043(0.87) & 12346 & 9769 & 4566\tabularnewline
\hline 
TEST2N5 & 14801(0.47) & 16097(0.77) & 12358(0.90) & 9440(0.93) & 4813(0.93)\tabularnewline
\hline 
TEST2N6 & 17444(0.23) & 16224(0.47) & 9103(0.73) & 7855(0.57) & 4410(0.53)\tabularnewline
\hline 
TEST2N7 & 22780(0.23) & 20330(0.47) & 12699(0.40) & 9773(0.50) & 4243(0.47)\tabularnewline
\hline 
TEST30N3 & 7814 & 7967 & 7010 & 6382 & 5014\tabularnewline
\hline 
TEST30N4 & 8014 & 7683 & 6568 & 6317 & 5090\tabularnewline
\hline 
\textbf{TOTAL} & \textbf{573980(0.87)} & \textbf{479026(0.96)} & \textbf{397519(0.96)} & \textbf{368460(0.96)} & \textbf{251350(0.97)}\tabularnewline
\hline 
\end{tabular}
\end{table}
\begin{figure}[H]
\includegraphics[scale=0.9]{boxplot_1to1}

\caption{Box - plot for the comparison between different number of processing
units. The propagation method was set to 1to1.\label{fig:boxplot1to1}}
\end{figure}
From this experiment, it is clear that the proposed technique drastically
reduces the required number of function calls as the parallel processing
units increase, while at the same time the average reliability of
the method in finding the global minimum also increases.

In addition, to determine the effect of the propagation mechanism
on the reliability and speed of the method, another comparative experiment
was performed, in which the number of parallel processing units was
set to 5 ($N_{I}$ = 5) and all propagation mechanisms were used.
The results for this experiment are presented in Table \ref{tab:propagationExperiment}.

\begin{table}[H]
\caption{Comparison of different propagation schemes. The number of processing
units was set to 5.\label{tab:propagationExperiment}}

\raggedright{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
Function & 1to1 & 1toN & Nto1 & NtoN\tabularnewline
\hline 
\hline 
BF1 & 10315 & 8408 & 9471 & 8647\tabularnewline
\hline 
BF2 & 9403 & 8024 & 10578 & 7730\tabularnewline
\hline 
BRANIN & 5170 & 4633 & 5203 & 5789\tabularnewline
\hline 
CIGAR10 & 38887 & 25035 & 35527 & 34258\tabularnewline
\hline 
CM4 & 12339 & 14195 & 12296 & 12565\tabularnewline
\hline 
DISCUS10 & 10989 & 6484 & 7667 & 6154\tabularnewline
\hline 
EASOM & 3513 & 3072 & 3496 & 3121\tabularnewline
\hline 
ELP10 & 11100 & 13027 & 9598 & 7091\tabularnewline
\hline 
EXP4 & 9556 & 10654 & 9517 & 10607\tabularnewline
\hline 
EXP16 & 21307 & 29289 & 23833 & 27307\tabularnewline
\hline 
EXP64 & 39731 & 11959 & 38191 & 26175\tabularnewline
\hline 
GKLS250 & 6853 & 6568 & 6966 & 7808\tabularnewline
\hline 
GKLS350 & 10095 & 10366 & 10400 & 9674\tabularnewline
\hline 
GRIEWANK2 & 8926 & 6022 & 7791 & 5432\tabularnewline
\hline 
POTENTIAL3 & 4756 & 4011 & 4591 & 4075\tabularnewline
\hline 
POTENTIAL5 & 4221 & 4002 & 4176 & 3870\tabularnewline
\hline 
HANSEN & 4031 & 3092 & 4320 & 3265\tabularnewline
\hline 
HARTMAN3 & 9711 & 10154 & 9316 & 11110\tabularnewline
\hline 
HARTMAN6 & 7194 & 6914(0.73) & 6760(0.97) & 11242(0.73)\tabularnewline
\hline 
RASTRIGIN & 5974 & 4077 & 5383 & 4804\tabularnewline
\hline 
ROSENBROCK4 & 12244 & 13930 & 11511 & 14710\tabularnewline
\hline 
ROSENBROCK8 & 18508 & 15423 & 16659 & 20848\tabularnewline
\hline 
SHEKEL5 & 7898(0.93) & 9604(0.90) & 7513(0.93) & 10657(0.67)\tabularnewline
\hline 
SHEKEL7 & 8717(0.97) & 12204 & 9404 & 10805(0.70)\tabularnewline
\hline 
SHEKEL10 & 8229(0.93) & 13418(0.93) & 9693 & 12677(0.83)\tabularnewline
\hline 
SINU4 & 12842 & 14757 & 13154 & 13376\tabularnewline
\hline 
SINU8 & 16375 & 22754 & 17026 & 20121\tabularnewline
\hline 
TEST2N4 & 9769 & 6633(0.97) & 10289 & 7483(0.83)\tabularnewline
\hline 
TEST2N5 & 9440(0.93) & 4819(0.93) & 8077(0.90) & 5429(0.80)\tabularnewline
\hline 
TEST2N6 & 7855(0.57) & 5358(0.77) & 8354(0.60) & 5574(0.43)\tabularnewline
\hline 
TEST2N7 & 9773(0.50) & 5183(0.33) & 7417(0.53) & 6312(0.40)\tabularnewline
\hline 
TEST30N3 & 6382 & 6538 & 6176 & 7462\tabularnewline
\hline 
TEST30N4 & 6317 & 6938 & 6473 & 6305\tabularnewline
\hline 
\textbf{TOTAL} & \textbf{368460(0.96)} & \textbf{327545(0.96)} & \textbf{356826(0.97)} & \textbf{352483(0.92)}\tabularnewline
\hline 
\end{tabular}
\end{table}
From the experimental results, it appears that the 1-to-N propagation
method has slightly better performances than the rest of the best
particle propagation techniques among the sub-populations.

The last experiment had to do with the effect of the $N_{P}$ parameter
on the speed of the method. In it, 5 parallel computing units were
used and the propagation method was set to \textbf{Nto1}. The experimental
results are presented in the Table \ref{tab:npresults}.

\begin{table}[H]
\caption{The effect of the parameter $N_{P}$ to the speed of the proposed
method. The number of threads was set to 5 and the value of $N_{P}$
was changed from 1 to 10.\label{tab:npresults} The propagation scheme
used was Nto1.}

\raggedright{}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Function & $N_{P}=1$ & $N_{P}=2$ & $N_{P}=3$ & $N_{P}=5$ & $N_{P}=10$\tabularnewline
\hline 
\hline 
BF1 & 10114 & 9976 & 10257 & 9471 & 9307\tabularnewline
\hline 
BF2 & 9224 & 10413 & 10051 & 10578 & 9530\tabularnewline
\hline 
BRANIN & 7037 & 6027 & 6238 & 5203 & 4851\tabularnewline
\hline 
CIGAR10 & 39244 & 35793 & 35811 & 35527 & 35835\tabularnewline
\hline 
CM4 & 11839 & 11588 & 12061 & 12296 & 11878\tabularnewline
\hline 
DISCUS10 & 8078 & 6950 & 9671 & 7667 & 11977\tabularnewline
\hline 
EASOM & 3669 & 3538 & 3589 & 3496 & 3477\tabularnewline
\hline 
ELP10 & 11656 & 12382 & 10643 & 9598 & 9641\tabularnewline
\hline 
EXP4 & 9257 & 9340 & 9395 & 9517 & 9626\tabularnewline
\hline 
EXP16 & 27275 & 24008 & 22906 & 23833 & 23190\tabularnewline
\hline 
EXP64 & 39679 & 37334 & 32126 & 38191 & 31119\tabularnewline
\hline 
GKLS250 & 7250 & 6893 & 7116 & 6966 & 6568\tabularnewline
\hline 
GKLS350 & 10281 & 9236 & 10088 & 10400 & 9552\tabularnewline
\hline 
GRIEWANK2 & 9259 & 10096 & 10297 & 7791 & 9527\tabularnewline
\hline 
POTENTIAL3 & 8471 & 6694 & 5770 & 4591 & 4829\tabularnewline
\hline 
POTENTIAL5 & 7127 & 5869 & 5301 & 4176 & 4844\tabularnewline
\hline 
HANSEN & 7978 & 6230 & 5591 & 4320 & 4573\tabularnewline
\hline 
HARTMAN3 & 10162 & 9939 & 10131 & 9316 & 10081\tabularnewline
\hline 
HARTMAN6 & 10614 & 9033 & 8059 & 6760 & 7507\tabularnewline
\hline 
RASTRIGIN & 7491 & 6384 & 6876 & 5383 & 5540\tabularnewline
\hline 
ROSENBROCK4 & 22600 & 16513 & 13631 & 11511 & 12169\tabularnewline
\hline 
ROSENBROCK8 & 34125 & 23004 & 21027 & 16659 & 19740\tabularnewline
\hline 
SHEKEL5 & 12299 & 11923 & 9521 & 7513 & 7256\tabularnewline
\hline 
SHEKEL7 & 13895 & 12358 & 10239 & 9404 & 9471\tabularnewline
\hline 
SHEKEL10 & 14130 & 11536 & 10235 & 9693 & 6936\tabularnewline
\hline 
SINU4 & 12760 & 12601 & 12268 & 13154 & 11905\tabularnewline
\hline 
SINU8 & 16957 & 17327 & 16346 & 17026 & 17030\tabularnewline
\hline 
TEST2N4 & 12215 & 9152 & 8136 & 10289 & 10024\tabularnewline
\hline 
TEST2N5 & 11384 & 8429 & 7934 & 8077 & 8935\tabularnewline
\hline 
TEST2N6 & 11833 & 8101 & 7825 & 8354 & 9655\tabularnewline
\hline 
TEST2N7 & 11162 & 8362 & 8692 & 7417 & 9486\tabularnewline
\hline 
TEST30N3 & 7178 & 7015 & 6829 & 6176 & 6216\tabularnewline
\hline 
TEST30N4 & 6518 & 6676 & 6509 & 6473 & 6756\tabularnewline
\hline 
\textbf{TOTAL} & \textbf{442761} & \textbf{390720} & \textbf{371169} & \textbf{356826} & \textbf{359031}\tabularnewline
\hline 
\end{tabular}
\end{table}
Increasing the value of the parameter from 1 to 5 drastically reduces
the required number of function calls, and this remains almost constant
for increasing the value for that parameter.

\section{Conclusions\label{sec:Conclusions}}

In this paper, a number of new ideas for parallel implementation of
the well-established particle optimization method were presented.
In the new method, a technique of propagating the best particles between
computing units as well as a termination rule of the overall process
were introduced. In the case of the propagation of the best particles
from the experiments carried out, it appears that it is more efficient
to send between the computing units more than the best particle. Furthermore,
the propagation method between parallel computing units did not have
a drastic effect on the efficiency and speed of the method, although
the 1-to-N propagation method appeared to have slightly better results.
However, the biggest gain from using the method lies in the increase
in parallel processing units. From the experiments performed, it is
evident that as parallel processing units increase, the total function
calls required to find the global minimum decreases. In addition,
the increase in parallel processing units improved to some extent
the efficiency of the method in finding the global minimum.

In the future, more and more effective termination techniques than
the proposed one should be developed and possibly better techniques
for propagating the best particles among computing units.

\vspace{6pt}


\authorcontributions{I.G.T. and V.C. conceived of the idea, the methodology implemented
the software. I.G.T. conducted the experiments, employing datasets,
and provided the comparative experiments. V.C. performed the statistical
analysis and prepared the manuscript. All authors have read and agreed
to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable. }

\institutionalreview{Not applicable.}

\acknowledgments{The experiments of this research work were performed at the high
performance computing system established at Knowledge and Intelligent
Computing Laboratory, Department of Informatics and Telecommunications,
University of Ioannina, acquired with the project “Educational Laboratory
equipment of TEI of Epirus” with MIS 5007094 funded by the Operational
Programme “Epirus” 2014--2020, by ERDF and national funds.}

\conflictsofinterest{The authors declare no conflict of interest.}

\sampleavailability{Not applicable.}

\appendixtitles{}

\appendixstart{}

\appendix

\begin{adjustwidth}{-\extralength}{0cm}{}

\reftitle{References}
\begin{thebibliography}{999}
\bibitem{go_physics1}L. Yang, D. Robin, F. Sannibale, C. Steier,
W. Wan, Global optimization of an accelerator lattice using multiobjective
genetic algorithms, Nuclear Instruments and Methods in Physics Research
Section A: Accelerators, Spectrometers, Detectors and Associated Equipment
\textbf{609}, pp. 50-57, 2009.

\bibitem{go_physics2}E. Iuliano, Global optimization of benchmark
aerodynamic cases using physics-based surrogate models, Aerospace
Science and Technology \textbf{67}, pp.273-286, 2017.

\bibitem{go_physics3}Q. Duan, S. Sorooshian, V. Gupta, Effective
and efficient global optimization for conceptual rainfall-runoff models,
Water Resources Research \textbf{28}, pp. 1015-1031 , 1992.

\bibitem{go_chem1}S. Heiles, R. L. Johnston, Global optimization
of clusters using electronic structure methods, Int. J. Quantum Chem.
\textbf{113}, pp. 2091-- 2109, 2013.

\bibitem{go_chem2}W.H. Shin, J.K. Kim, D.S. Kim, C. Seok, GalaxyDock2:
Protein--ligand docking using beta-complex and global optimization,
J. Comput. Chem. \textbf{34}, pp. 2647-- 2656, 2013.

\bibitem{go_chem3}A. Liwo, J. Lee, D.R. Ripoll, J. Pillardy, H. A.
Scheraga, Protein structure prediction by global optimization of a
potential energy function, Biophysics \textbf{96}, pp. 5482-5485,
1999.

\bibitem{go_econ1}Zwe-Lee Gaing, Particle swarm optimization to solving
the economic dispatch considering the generator constraints, IEEE
Transactions on \textbf{18} Power Systems, pp. 1187-1195, 2003.

\bibitem{go_econ2}C. D. Maranas, I. P. Androulakis, C. A. Floudas,
A. J. Berger, J. M. Mulvey, Solving long-term financial planning problems
via global optimization, Journal of Economic Dynamics and Control
\textbf{21}, pp. 1405-1425, 1997.

\bibitem{go_med1}Eva K. Lee, Large-Scale Optimization-Based Classification
Models in Medicine and Biology, Annals of Biomedical Engineering \textbf{35},
pp 1095-1109, 2007.

\bibitem{go_med2}Y. Cherruault, Global optimization in biology and
medicine, Mathematical and Computer Modelling \textbf{20}, pp. 119-132,
1994.

\bibitem{interval1}M.A. Wolfe, Interval methods for global optimization,
Applied Mathematics and Computation \textbf{75}, pp. 179-206, 1996.

\bibitem{interval2}T. Csendes and D. Ratz, Subdivision Direction
Selection in Interval Methods for Global Optimization, SIAM J. Numer.
Anal. \textbf{34}, pp. 922--938, 1997. 

\bibitem{crs1}W. L. Price, Global optimization by controlled random
search, Journal of Optimization Theory and Applications \textbf{40},
pp. 333-348, 1983.

\bibitem{crs2}Ivan Křivý, Josef Tvrdík, The controlled random search
algorithm in optimizing regression models, Computational Statistics
\& Data Analysis \textbf{20}, pp. 229-234, 1995.

\bibitem{crs3}M.M. Ali, A. Törn, and S. Viitanen, A Numerical Comparison
of Some Modified Controlled Random Search Algorithms, Journal of Global
Optimization \textbf{11},pp. 377--385,1997.

\bibitem{simann_major}S. Kirkpatrick, CD Gelatt, , MP Vecchi, Optimization
by simulated annealing, Science \textbf{220}, pp. 671-680, 1983.

\bibitem{simann1}L. Ingber, Very fast simulated re-annealing, Mathematical
and Computer Modelling \textbf{12}, pp. 967-973, 1989.

\bibitem{simann2}R.W. Eglese, Simulated annealing: A tool for operational
research, Simulated annealing: A tool for operational research \textbf{46},
pp. 271-281, 1990.

\bibitem{diffe1}R. Storn, K. Price, Differential Evolution - A Simple
and Efficient Heuristic for Global Optimization over Continuous Spaces,
Journal of Global Optimization \textbf{11}, pp. 341-359, 1997.

\bibitem{diffe2}J. Liu, J. Lampinen, A Fuzzy Adaptive Differential
Evolution Algorithm. Soft Comput \textbf{9}, pp.448--462, 2005.

\bibitem{pso_major}J. Kennedy and R. Eberhart, \textquotedbl Particle
swarm optimization,\textquotedbl{} Proceedings of ICNN'95 - International
Conference on Neural Networks, 1995, pp. 1942-1948 vol.4, doi: 10.1109/ICNN.1995.488968.

\bibitem{pso1}Riccardo Poli, James Kennedy kennedy, Tim Blackwell,
Particle swarm optimization An Overview, Swarm Intelligence \textbf{1},
pp 33-57, 2007. 

\bibitem{pso2}Ioan Cristian Trelea, The particle swarm optimization
algorithm: convergence analysis and parameter selection, Information
Processing Letters \textbf{85}, pp. 317-325, 2003.

\bibitem{aco1}M. Dorigo, M. Birattari and T. Stutzle, Ant colony
optimization, IEEE Computational Intelligence Magazine \textbf{1},
pp. 28-39, 2006.

\bibitem{aco2}K. Socha, M. Dorigo, Ant colony optimization for continuous
domains, European Journal of Operational Research 185, pp. 1155-1173,
2008.

\bibitem{ga1}D. Goldberg, Genetic Algorithms in Search, Optimization
and Machine Learning, Addison-Wesley Publishing Company, Reading,
Massachussets, 1989.

\bibitem{ga2}Z. Michaelewicz, Genetic Algorithms + Data Structures
= Evolution Programs. Springer - Verlag, Berlin, 1996.

\bibitem{ga3}S.A. Grady, M.Y. Hussaini, M.M. Abdullah, Placement
of wind turbines using genetic algorithms, Renewable Energy \textbf{30},
pp. 259-270, 2005.

\bibitem{gpu1}Y. Zhou and Y. Tan, \textquotedbl GPU-based parallel
particle swarm optimization,\textquotedbl{} 2009 IEEE Congress on
Evolutionary Computation, pp. 1493-1500, 2009.

\bibitem{gpu2}L. Dawson and I. Stewart, \textquotedbl Improving
Ant Colony Optimization performance on the GPU using CUDA,\textquotedbl{}
2013 IEEE Congress on Evolutionary Computation, 2013, pp. 1901-1908,
doi: 10.1109/CEC.2013.6557791.

\bibitem{gpu3}Barkalov, K., Gergel, V. Parallel global optimization
on GPU. J Glob Optim 66, 3--20 (2016). 

\bibitem{meta1}I. BoussaïD, J. Lepagnot, P. Siarry, P., A survey
on optimization metaheuristics. Information sciences \textbf{237},
pp. 82-117, 2013.

\bibitem{meta2}T. Dokeroglu, E. Sevinc, T. Kucukyilmaz, A. Cosar,
A survey on new generation metaheuristic algorithms. Computers \&
Industrial Engineering \textbf{137}, 106040, 2019.

\bibitem{meta3}K. Hussain, M.N.M. Salleh, S. Cheng, Y. Shi, Metaheuristic
research: a comprehensive survey.Artificial Intelligence Review \textbf{52},
pp. 2191-2233, 2019.

\bibitem{psophysics1}Anderson Alvarenga de Moura Meneses, Marcelo
Dornellas, Machado Roberto Schirru, Particle Swarm Optimization applied
to the nuclear reload problem of a Pressurized Water Reactor, Progress
in Nuclear Energy \textbf{51}, pp. 319-326, 2009.

\bibitem{psophysics2}Ranjit Shaw, Shalivahan Srivastava, Particle
swarm optimization: A new tool to invert geophysical data, Geophysics
\textbf{72}, 2007.

\bibitem{psochem1}C. O. Ourique, E.C. Biscaia, J.C. Pinto, The use
of particle swarm optimization for dynamical analysis in chemical
processes, Computers \& Chemical Engineering \textbf{26}, pp. 1783-1793,
2002.

\bibitem{psochem2}H. Fang, J. Zhou, Z. Wang et al, Hybrid method
integrating machine learning and particle swarm optimization for smart
chemical process operations, Front. Chem. Sci. Eng. \textbf{16}, pp.
274--287, 2022.

\bibitem{psomed1}M.P. Wachowiak, R. Smolikova, Yufeng Zheng, J.M.
Zurada, A.S. Elmaghraby, An approach to multimodal biomedical image
registration utilizing particle swarm optimization, IEEE Transactions
on Evolutionary Computation \textbf{8}, pp. 289-301, 2004.

\bibitem{psomed2}Yannis Marinakis. Magdalene Marinaki, Georgios Dounias,
Particle swarm optimization for pap-smear diagnosis, Expert Systems
with Applications \textbf{35}, pp. 1645-1656, 2008. 

\bibitem{psoecon}Jong-Bae Park, Yun-Won Jeong, Joong-Rin Shin, Kwang
Y. Lee, An Improved Particle Swarm Optimization for Nonconvex Economic
Dispatch Problems, IEEE Transactions on Power Systems \textbf{25},
pp. 156-16\textbf{216}6, 2010.

\bibitem{pso_mutation1}A. Stacey, M. Jancic, I. Grundy, Particle
swarm optimization with mutation, In: 2003 Congress on Evolutionary
Computation, 2003. CEC '03., pp. 1425-1430, 2003.

\bibitem{pso_mutation2}M. Pant, R. Thangaraj, A. Abraham, Particle
Swarm Optimization Using Adaptive Mutation, In: 2008 19th International
Workshop on Database and Expert Systems Applications, pp. 519-523,
2008.

\bibitem{pso_mutation3}N. Higashi, H. Iba, Particle swarm optimization
with Gaussian mutation, In: Proceedings of the 2003 IEEE Swarm Intelligence
Symposium. SIS'03 (Cat. No.03EX706), pp. 72-79, 2003.

\bibitem{pso_initvelocity}A. Engelbrecht, \textquotedbl Particle
swarm optimization: Velocity initialization,\textquotedbl{} 2012 IEEE
Congress on Evolutionary Computation, pp. 1-8, 2012.

\bibitem{psohybrid1}B. Liu, L. Wang, Y.H. Jin, F. Tang, D.X. Huang,
Improved particle swarm optimization combined with chaos, Chaos Solitons
and Fractals \textbf{25}, pp. 1261-1271, 2005.

\bibitem{psohybrid2}X.H. Shi, Y.C. Liang, H.P. Lee, C. Lu, L.M. Wang,
An improved GA and a novel PSO-GA based hybrid algorithm, Information
Processing Letters \textbf{93}, pp. 255-261, 2005.

\bibitem{psohybrid3}Harish Garg, A hybrid PSO-GA algorithm for constrained
optimization problems, Applied Mathematics and Computation \textbf{274},
pp. 292-305, 2016.

\bibitem{pso_parallel1}J. F. Schutte, J. A. Reinbolt, B. J. Fregly,
R. T. Haftka, A. D. George, Parallel global optimization with the
particle swarm algorithm, Int. J. Numer. Meth. Engng. \textbf{61},
pp. 2296-2315, 2004.

\bibitem{pso_parallel2}B-Il Koh, A.D. George, R.T. Haftka, B.J. Fregly,
Parallel asynchronous particle swarm optimization. Int. J. Numer.
Meth. Engng., \textbf{67}, pp. 578-595, 2006.

\bibitem{pso_parallel3}G. Venter, J. Sobieszczanski-Sobieski, Parallel
Particle Swarm Optimization Algorithm Accelerated by Asynchronous
Evaluations, Journal of Aerospace Computing, Information, and Communication
\textbf{3}, pp. 123-137, 2006.

\bibitem{pso_velocity1}Z.L. Gaing, Particle swarm optimization to
solving the economic dispatch considering the generator constraints,
IEEE Transactions on Power Systems \textbf{18}, pp. 1187-1195, 2003.

\bibitem{pso_velocity2}X. Yang, Jinsha Yuan, Jiangy Yuan, H. Mao,
A modified particle swarm optimizer with dynamic adaptation, Applied
Mathematics and Computation \textbf{189}, pp. 1205-1213, 2007.

\bibitem{pso_velocity3}Y. Jiang, T. Hu, C. Huang, X. Wu, An improved
particle swarm optimization algorithm, Applied Mathematics and Computation
\textbf{193}, pp. 231-239, 2007.

\bibitem{ge_pso1}A. Bogdanova, J.P. Junior, C. Aranha, Franken-Swarm:
Grammatical Evolution for the Automatic Generation of Swarm-like Meta-Heuristics,
In: Proceedings of the Genetic and Evolutionary Computation Conference
Companion, pp. 411-412, 2019.

\bibitem{ge_mainpaper}M. O'Neill, C. Ryan, Grammatical evolution,IEEE
Transactions on Evolutionary Computation \textbf{5}, pp. 349-358,
2001.

\bibitem{siman_pso}X. Pan, L. Xue, Y. Lu et al, Hybrid particle swarm
optimization with simulated annealing, Multimed Tools Appl \textbf{78},
pp. 29921--29936, 2019.

\bibitem{simman_pso_energy}M.A. Mughal, Q. Ma, C. Xiao, Photovoltaic
Cell Parameter Estimation Using Hybrid Particle Swarm Optimization
and Simulated Annealing, Energies \textbf{10}, 2017.

\bibitem{pso_de}G.H. Lin, J. Zhang, Z.H. Liu, Hybrid particle swarm
optimization with differential evolution for numerical and engineering
optimization. Int. J. Autom. Comput. \textbf{15}, pp. 103--114, 2018.

\bibitem{pso_local1}S. Li, M. Tan, I. W. Tsang, J. T. -Y. Kwok, A
Hybrid PSO-BFGS Strategy for Global Optimization of Multimodal Functions,
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)
\textbf{41}, pp. 1003-1014, 2011.

\bibitem{pso_local2}G. Wu, D. Qiu, Y. Yu, W. Pedrycz, M. Ma, H. Li,
Superior solution guided particle swarm optimization combined with
local search techniques, Expert Systems with Applications \textbf{41},
pp. 7536-7548, 2014.

\bibitem{ipso}V. Charilogis, I.G. Tsoulos, Toward an Ideal Particle
Swarm Optimizer for Multidimensional Functions, Information \textbf{13},
217, 2022.

\bibitem{Powell}M.J.D Powell, A Tolerant Algorithm for Linearly Constrained
Optimization Calculations, Mathematical Programming \textbf{45}, pp.
547-566, 1989. 

\bibitem{Ali1}M. Montaz Ali, Charoenchai Khompatraporn, Zelda B.
Zabinsky, A Numerical Evaluation of Several Stochastic Algorithms
on Selected Continuous Global Optimization Test Problems, Journal
of Global Optimization \textbf{31}, pp 635-672, 2005. 

\bibitem{Floudas1}C.A. Floudas, P.M. Pardalos, C. Adjiman, W. Esposoto,
Z. G$\ddot{\mbox{u}}$m$\ddot{\mbox{u}}$s, S. Harding, J. Klepeis,
C. Meyer, C. Schweiger, Handbook of Test Problems in Local and Global
Optimization, Kluwer Academic Publishers, Dordrecht, 1999.

\bibitem{testfunctions1}M.M. Ali and P. Kaelo, Improved particle
swarm algorithms for global optimization, Applied Mathematics and
Computation \textbf{196}, pp. 578-593, 2008.

\bibitem{testfunctions2}H. Koyuncu, R. Ceylan, A PSO based approach:
Scout particle swarm algorithm for continuous global optimization
problems, Journal of Computational Design and Engineering \textbf{6},
pp. 129--142, 2019.

\bibitem{testfunctions3}Patrick Siarry, Gérard Berthiau, François
Durdin, Jacques Haussy, ACM Transactions on Mathematical Software
\textbf{23}, pp 209--228, 1997.

\bibitem{testfunctions4}I.G. Tsoulos, I.E. Lagaris, GenMin: An enhanced
genetic algorithm for global optimization, Computer Physics Communications\textbf{
178, }pp. 843-851, 2008.

\bibitem{gkls}M. Gaviano, D.E. Ksasov, D. Lera, Y.D. Sergeyev, Software
for generation of classes of test functions with known local and global
minima for global optimization, ACM Trans. Math. Softw. \textbf{29},
pp. 469-480, 2003.

\bibitem{Jones}J.E. Lennard-Jones, On the Determination of Molecular
Fields, Proc. R. Soc. Lond. A \textbf{ 106}, pp. 463--477, 1924.

\end{thebibliography}

\end{adjustwidth}{}
\end{document}
